<link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;  */
		font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	tr.spaceUnder>td {
  		padding-bottom: 4em;
	}

</style>

<html>
<head>
	<title>The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from a Single Image</title>
	<meta property="og:image" content="Path to my teaser.png"/>
	<meta property="og:title" content="The augmented image prior: Distilling 1000 classes by extrapolating from a single image" />
	<meta property="og:description" content="What can neural networks learn about the visual world when provided with only a single image as input? While any image obviously cannot contain the multitudes of all existing objects, scenes and lighting conditions -- within the space of all  256<sup>3x224x224</sup> possible 224-sized square images, it might still provide a strong prior for natural images. To analyze this ``augmented image prior'' hypothesis, we develop a simple framework for training neural networks from scratch using a single image and augmentations using knowledge distillation from a supervised pretrained teacher. With this, we find the answer to the above question to be: `surprisingly, a lot'.  In quantitative terms, we find accuracies of 94%/74% on CIFAR-10/100, 69% on ImageNet, and by extending this method to video and audio, 51% on Kinetics-400 and 84% on SpeechCommands. In extensive analyses spanning 13 datasets, we disentangle the effect of augmentations, choice of data and network architectures and also provide qualitative evaluations that include lucid ``panda neurons'' in networks that have never even seen one."/>
</head>

<body>
	<br>
	<center>
		<span style="font-size:32px">The Augmented Image Prior: <br> Distilling 1000 Classes by Extrapolating from a Single Image</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://yukimasano.github.io/">Yuki M. Asano*</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://aqibsaeed.github.io/">Aaqib Saeed*</a></span>
						</center>
					</td>
				</tr>
			</table>
			<br>
			<table align=center>
				<tr>
					<td align=center>*Equal Contribution</td>
				</tr>
			</table>
			<table align=center>
				<tr>
					<td align=center>ICLR 2023 Paper</td>
				</tr>
			</table>
			<br>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href='http://www.arxiv.org/abs/2112.00725'>[Paper]</a></a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/yukimasano/single-img-extrapolating'>[Code & pretrained models]</a></span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	
	<br>
	<center>
		<table align=center width=950px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:800px" src="./resources/animation_final.gif"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					<b>Extrapolating from one image.</b> Strongly augmented patches from a single image are used to train a student (S) to distinguish semantic classes, such as those in ImageNet.
					The student neural network is initialized randomly and learns from a pretrained teacher (T) via  KL-divergence. 
					Although almost none of target categories are present in the image, we find student performances of > 69% for classifying ImageNet's 1000 classes.
					In this paper, we develop this single datum learning framework and investigate it across datasets and domains.
				</td>
			</tr>
		</table>
	</center>
	<hr>
	<table align=center width=850px>
		<center><h1>Key contributions</h1></center>
		<tr>
			<td>
				<ul>
					<li>A minimal framework for training neural networks with a single datum from scratch using distillation.</li>
					<li>Extensive ablations of the proposed method, such as the dependency on the source image, the choice of augmentations and network architectures.</li>
					<li>Large scale empirical evidence of neural networks' ability to extrapolate on > 12 vision and audio datasets.</li>
					<li>Qualitative insights on what and how neural networks trained with a single image learn.</li>
				</ul>
			</td>
		</tr>
	</table>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				What can neural networks learn about the visual world when provided with only a single image as input? While any image obviously cannot contain the multitudes of all existing objects, scenes and lighting conditions – within the space of all  256<sup>3x224x224</sup> possible 224-sized square images, it might still provide a strong prior for natural images. To analyze this <i>augmented image prior</i> hypothesis, we develop a simple framework for training neural networks from scratch using a single image and augmentations using knowledge distillation from a supervised pretrained teacher. With this, we find the answer to the above question to be: <i>surprisingly, a lot</i>.  In quantitative terms, we find accuracies of 94%/74% on CIFAR-10/100, 69% on ImageNet, and by extending this method to video and audio, 51% on Kinetics-400 and 84% on SpeechCommands. In extensive analyses spanning 13 datasets, we disentangle the effect of augmentations, choice of data and network architectures and also provide qualitative evaluations that include lucid <i>panda neurons</i> in networks that have never even seen one.
			</td>
		</tr>
	</table>
	<br>

	<hr>

	<table align=center width=850px>
		<center><h1>Talk</h1></center>
		<tr>
			<td>
				<center><iframe width="560" height="315" src="https://www.youtube.com/embed/I0YycIndk7M" title="YouTube video player" 
		frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
					</center></td>
		</tr>
	</table>
	<br>

        <hr>

	<table align=center width=850px>
		<center><h1>Selected Results</h1></center>
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Distilling dataset.</b> 1 image + augmentations ≈ almost 50K in-domain CIFAR-10/100images.</p>
					<img class="round" style="width:400px" src="./resources/tab_1.png"/>
				</center>
			</td>
		</tr>
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Distilling source image.</b> Content matters: less dense images do not train as well.</p>
					<img class="round" style="width:250px" src="./resources/distilling_source_image.PNG"/>
				</center>
			</td>
		</tr>
		
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Distilling audio representations.</b> Our approach also generalizes for audio by using 1 audio clip + augmentations.</p>
					<img class="round" style="width:550px" src="./resources/tab_7_wo_ref.png"/>
				</center>
			</td>
		</tr>

		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Larger scale datasets.</b> Our method scales to larger models utilizing 224×224-sized images.</p>
					<img class="round" style="width:550px" src="./resources/large_scale_datasets.PNG"/>
				</center>
			</td>
		</tr>
		
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Analysis of IN-1k model distillations.</b> We vary distillation dataset and teacher and student configs. We achieve 69% top-1 single-crop accuracy on IN-1k. Even with just argmax (AM) signal, performance remains high at 44%. See more details in the paper.</p>
					<img class="round" style="width:500px" src="./resources/in1k_distillations.PNG"/>
				</center>
			</td>
		</tr>
	
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Varying student width or depth.</b> We find wide models benefit more from parameter-increase an even reach the teacher's performance on ImageNet.</p>
					<img class="round" style="width:500px" src="./resources/scaling_params_in1k.png"/>
				</center>
			</td>
		</tr>
		
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Visualizing neurons.</b> We find neurons that fire for objects the network has never seen.</p>
					<img class="round" style="width:500px" src="./resources/fig_7.png"/>
				</center>
			</td>
		</tr>
	</table>

	<br>

	<hr>

	<table align=center width=850px>
		<center><h1>Our training data</h1></center>
		<tr>
			<td>
				       <img class="round"style="width:49.5%" src="./resources/patches.png"/>
					   <img class="round" style="width:49.5%" src="./resources/vid.gif"/>
				<p><b>Training data.</b> We generate a dataset from a single datum and use it for training networks from scratch.</p>
			</td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Y. M. Asano, A. Saeed<br>
				<b>The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from a Single Image</b><br>
				(<a href="http://www.arxiv.org/abs/2112.00725">ICLR 2023</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="https://yukimasano.github.io/data/asano2023extrapolating.bib">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>
 
	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					Y.M.A. is thankful for MLRA funding from AWS. We also thank T. Blankevoort, A.F. Biten and S. Albanie for useful comments on a draft of this paper.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

