<link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		/* font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;  */
		font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	tr.spaceUnder>td {
  		padding-bottom: 1em;
	}

</style>

<html>
<head>
	<title>Extrapolating from a Single Image to a Thousand Classes using Distillation</title>
	<meta property="og:image" content="Path to my teaser.png"/>
	<meta property="og:title" content="Extrapolating from a Single Image to a Thousand Classes using Distillation" />
	<meta property="og:description" content="What can neural networks learn about the visual world from a single image? While it obviously cannot contain the multitudes of possible objects, scenes and lighting conditions that exist-within the space of all possible 256<sup>3x224x224</sup> 224-sized square images, it might still provide a strong prior for natural images. 
		To analyze this hypothesis, we develop a framework for training neural networks from scratch using a single image by means of knowledge distillation from a supervised pretrained teacher. 
		With this, we find that the answer to the above question is: surprisingly, a lot. In quantitative terms, we find top-1 accuracies of 94%/74% on CIFAR-10/100, 59% on ImageNet and, by extending this method to audio, 84% on SpeechCommands. 
		In extensive analyses we disentangle the effect of augmentations, choice of source image and network architectures and also discover panda neurons in networks that have never seen a panda. 
		This work shows that one image can be used to extrapolate to thousands of object classes and motivates a renewed research agenda on the fundamental interplay of augmentations and images." />
</head>

<body>
	<br>
	<center>
		<span style="font-size:32px">Extrapolating from a Single Image to a Thousand Classes using Distillation</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://yukimasano.github.io/">Yuki M. Asano*</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://aqibsaeed.github.io/">Aaqib Saeed*</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center>
				<tr>
					<td align=center>*Equal Contribution</td>
				</tr>
			</table>
			<br>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='http://www.arxiv.org/abs/2112.00725'>[Paper]</a></a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/yukimasano/single-img-extrapolating'>[Code & Pretrained models]</a></span>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	
	<br>
	<center>
		<table align=center width=950px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:800px" src="./resources/animation_final.gif"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					<b>Extrapolating from one image.</b> Strongly augmented patches from a single image are used to train a student (S) to distinguish semantic classes, such as those in ImageNet.
					The student neural network is initialized randomly and learns from a pretrained teacher (T) via  KL-divergence. 
					Although almost none of target categories are present in the image, we find student performances of > 59% for classifying ImageNet's 1000 classes.
					In this paper, we develop this single datum learning framework and investigate it across datasets and domains.
				</td>
			</tr>
		</table>
	</center>
	<hr>
	<table align=center width=850px>
		<center><h1>Key contributions</h1></center>
		<tr>
			<td>
				<ul>
					<li>A minimal framework for training neural networks with a single datum from scratch using distillation.</li>
					<li>Extensive ablations of the proposed method, such as the dependency on the source image, the choice of augmentations and network architectures.</li>
					<li>Large scale empirical evidence of neural networks' ability to extrapolate on > 12 vision and audio datasets.</li>
					<li>Qualitative insights on what and how neural networks trained with a single image learn.</li>
				</ul>
			</td>
		</tr>
	</table>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				What can neural networks learn about the visual world from a single image? While it obviously cannot contain the multitudes of possible objects, scenes and lighting conditions that exist&#8211;within the space of all possible 256<sup>3x224x224</sup> 224-sized square images, it might still provide a strong prior for natural images. 
				To analyze this hypothesis, we develop a framework for training neural networks from scratch using a single image by means of knowledge distillation from a supervised pretrained teacher. 
				With this, we find that the answer to the above question is: <i>surprisingly, a lot</i>. In quantitative terms, we find top-1 accuracies of 94%/74% on CIFAR-10/100, 59% on ImageNet and, by extending this method to audio, 84% on SpeechCommands. 
				In extensive analyses we disentangle the effect of augmentations, choice of source image and network architectures and also discover <i>panda neurons</i> in networks that have never seen a panda. 
				This work shows that one image can be used to extrapolate to thousands of object classes and motivates a renewed research agenda on the fundamental interplay of augmentations and images.
			</td>
		</tr>
	</table>
	<br>

	<hr>

	<table align=center width=850px>
		<center><h1>Selected Results</h1></center>
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Distilling dataset.</b> 1 image + augmentations â‰ˆ almost 50K in-domain CIFAR-10/100images.</p>
					<img class="round" style="width:400px" src="./resources/tab_1.png"/>
				</center>
			</td>
		</tr>
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Distilling source image.</b> Content matters: less dense images do not train as well.</p>
					<img class="round" style="width:400px" src="./resources/tab_2a.png"/>
				</center>
			</td>
		</tr>

		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Distilling audio representations.</b> Our approach also generalizes for audio by using 1 audio clip + augmentations.</p>
					<img class="round" style="width:550px" src="./resources/tab_7_wo_ref.png"/>
				</center>
			</td>
		</tr>

		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Analysis of IN-1k model distillations.</b> We vary distillation dataset and teacher and student configs. We achieve <u>59.1% top-1 single-crop accuracy</u> on IN-1k. See more details in the paper.</p>
					<img class="round" style="width:500px" src="./resources/tab_9.png"/>
				</center>
			</td>
		</tr>
		<tr class="spaceUnder">
			<td>
				<center>
					<p><b>Visualizing neurons.</b> We find neurons that fire for objects the network has never seen.</p>
					<img class="round" style="width:500px" src="./resources/fig_7.png"/>
				</center>
			</td>
		</tr>
	</table>

	<br>

	<hr>

	<table align=center width=850px>
		<center><h1>Our training data</h1></center>
		<tr>
			<td>
				       <img class="round"style="width:49.5%" src="./resources/patches.png"/>
					   <img class="round" style="width:49.5%" src="./resources/vid.gif"/>
				<p><b>Training data.</b> We generate a dataset from a single datum and use it for training networks from scratch.</p>
			</td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Y. M. Asano, A. Saeed<br>
				<b>Extrapolating from a Single Image to a Thousand Classes using Distillation</b><br>
				(hosted on <a href="http://www.arxiv.org/abs/2112.00725">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>
 
	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					Y.M.A. is thankful for MLRA funding from AWS. We also thank T. Blankevoort, A.F. Biten and S. Albanie for useful comments on a draft of this paper.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

